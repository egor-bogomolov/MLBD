{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Movie-recommendation\" data-toc-modified-id=\"Movie-recommendation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Movie recommendation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Evaluation-Protocol\" data-toc-modified-id=\"Evaluation-Protocol-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Evaluation Protocol</a></span></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#ALS\" data-toc-modified-id=\"ALS-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-collaborative-filtering.html#explicit-vs-implicit-feedback\" target=\"_blank\">ALS</a></a></span></li><li><span><a href=\"#Ваша-формулировка\" data-toc-modified-id=\"Ваша-формулировка-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Ваша формулировка</a></span></li></ul></li><li><span><a href=\"#Evaluation-Results\" data-toc-modified-id=\"Evaluation-Results-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Evaluation Results</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation\n",
    "\n",
    "Ваша задача - рекомендация фильмов для пользователей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"spark_sql_examples\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"25g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "`MovieLens-25M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/egor/MLBD/data/ml-25m'\n",
    "\n",
    "RATINGS_PATH = os.path.join(DATA_PATH, 'ratings.csv')\n",
    "MOVIES_PATH = os.path.join(DATA_PATH, 'movies.csv')\n",
    "TAGS_PATH = os.path.join(DATA_PATH, 'tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "ratings_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, userId: string, movieId: string, rating: string, timestamp: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000095"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = ratings_df.count()\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162541"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = ratings_df \\\n",
    "    .select('userId') \\\n",
    "    .distinct() \\\n",
    "    .count()\n",
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(timestamp)=789652009, max(timestamp)=1574327703)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.select(F.min(F.col('timestamp')), F.max(F.col('timestamp'))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Protocol\n",
    "\n",
    "Так как мы хотим оценивать качество разных алгоритмов рекомендаций, то в первую очередь нам нужно определить\n",
    "* Как разбить данные на `Train`/`Validation`/`Test`;\n",
    "* Какие метрики использовать для оценки качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор методики разбиения датасета\n",
    "\n",
    "Будем рассматривать две опции:\n",
    "* Разбить все оценки во времени на три группы (train/valid/test)\n",
    "* Разбить все оценки на сессии (серия оценок от одного пользователя), каждую из них разбить во времени\n",
    "\n",
    "Второй способ хуже тем, что в нем мы заглядываем в будущее для предсказания оценок некоторых пользователей. Тем не менее, он решает проблему холодного старта (в первом способе некоторые пользователи встречаются в тестовой выборке, но при этом отсутствуют в тренировочной), из-за которой нам придется сократить тестовую выборку.\n",
    "\n",
    "Значит, чтобы выбрать методику разбиения, попробуем разбить датасет первым способом и поймем, какое количество оценок и пользователей придется выкинуть из-за холодного старта. Затем решим, насколько это количество критично и стоит ли переходить к сессиям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "def split_by_col(df: pyspark.sql.dataframe.DataFrame, split_col: str, parts_fractions: List[float]):\n",
    "    \"\"\"\n",
    "    df - DataFrame\n",
    "    split_col - total order column\n",
    "    parts_fractions - fractions of resulting parts\n",
    "    \"\"\"\n",
    "    \n",
    "    split_window = Window().orderBy(split_col)\n",
    "    df = df.withColumn('fraction', F.percent_rank().over(split_window))\n",
    "    \n",
    "    parts_fractions = [0.] + [sum(parts_fractions[:(i + 1)]) for i in range(len(parts_fractions))]\n",
    "    parts_fractions[-1] += 1e-9 # to allow weak inequality for the last part\n",
    "    parts = [\n",
    "        df.filter((low_fraction <= F.col('fraction')) & (F.col('fraction') < high_fraction)).drop('fraction')\n",
    "        for low_fraction, high_fraction in zip(parts_fractions[:-1], parts_fractions[1:])\n",
    "    ]\n",
    "    \n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = split_by_col(ratings_df, 'timestamp', [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем границу, отделяющую тренировочную выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_separator = train_df.select(F.max(F.col('timestamp'))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_separator = train_time_separator[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1466837397"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time_separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем долю пользователей и рейтингов, которые нам придется выкинуть в валидационной и тестовой выборках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_info = ratings_df \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(F.min(F.col('timestamp')).alias('minTimestamp'), F.count('movieId').alias('movieCount')) \\\n",
    "    .select('userId', 'minTimestamp', 'movieCount') \\\n",
    "    .filter(F.col('minTimestamp') > train_time_separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|userId|minTimestamp|movieCount|\n",
      "+------+------------+----------+\n",
      "|   471|  1499822567|        39|\n",
      "|   833|  1467556963|       158|\n",
      "|  1088|  1501273541|        36|\n",
      "|  1238|  1495751304|       150|\n",
      "|  3794|  1484584684|       336|\n",
      "|  4900|  1526377075|       337|\n",
      "|  6357|  1485048811|       241|\n",
      "|  7754|  1485014125|        42|\n",
      "|  7833|  1546799783|        33|\n",
      "|  9900|  1482079056|        63|\n",
      "| 10206|  1497044862|       110|\n",
      "| 10623|  1521403955|      1629|\n",
      "| 10817|  1554696855|        24|\n",
      "| 11141|  1509496887|        83|\n",
      "| 11317|  1549388861|        22|\n",
      "| 11458|  1472091641|        72|\n",
      "| 12940|  1504635908|        32|\n",
      "| 13832|  1494019772|        69|\n",
      "| 13840|  1542844561|       102|\n",
      "| 14450|  1532874678|       304|\n",
      "+------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24658"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_info.show()\n",
    "missed_info.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_info = missed_info \\\n",
    "    .select(F.count('userId').alias('missedUsers'), F.sum('movieCount').alias('missedRatings')) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(missedUsers=24658, missedRatings=4235184)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8470335812723911"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_info[0]['missedRatings'] / int(0.2 * dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_train_users = valid_df \\\n",
    "    .union(test_df) \\\n",
    "    .select('userId') \\\n",
    "    .distinct() \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7939083679448791"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_info[0]['missedUsers'] / non_train_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод\n",
    "\n",
    "Если мы будем делить датасет во времени, то мы потеряем 85% рейтингов в тестовой и валидационной выборках и почти 80% пользователей. Это очень существенное изменение размера датасета, поэтому разобьем датасет по сессиям, а уже их по времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitioned_split_by_col(\n",
    "    df: pyspark.sql.dataframe.DataFrame, split_col: str, partition_col: str, parts_fractions: List[float]\n",
    "):\n",
    "    \"\"\"\n",
    "    df - DataFrame\n",
    "    split_col - total order column\n",
    "    parts_fractions - fractions of resulting parts\n",
    "    \"\"\"\n",
    "    \n",
    "    split_window = Window().orderBy(split_col).partitionBy(partition_col)\n",
    "    df = df.withColumn('fraction', F.percent_rank().over(split_window))\n",
    "    \n",
    "    parts_fractions = [0.] + [sum(parts_fractions[:(i + 1)]) for i in range(len(parts_fractions))]\n",
    "    parts_fractions[-1] += 1e-9 # to allow weak inequality for the last part\n",
    "    parts = [\n",
    "        df.filter((low_fraction <= F.col('fraction')) & (F.col('fraction') < high_fraction)).drop('fraction')\n",
    "        for low_fraction, high_fraction in zip(parts_fractions[:-1], parts_fractions[1:])\n",
    "    ]\n",
    "    \n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = partitioned_split_by_col(ratings_df, 'timestamp', 'userId', [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8006953973574901, 0.09911926334679928, 0.10018533929571068)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count() / dataset_size, valid_df.count() / dataset_size, test_df.count() / dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162541, 158503, 158843)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select('userId').distinct().count(), \\\n",
    "valid_df.select('userId').distinct().count(), \\\n",
    "test_df.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор метрик для оценки качества\n",
    "\n",
    "Чтобы понять, какие метрики адекватно использовать, надо сперва понять, сколько фильмов посмотрели люди в тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts = test_df \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(F.count('movieId').alias('movieCount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(movieCount)=1, avg(movieCount)=15.768041399369189, max(movieCount)=3191)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts.select(F.min('movieCount'), F.mean('movieCount'), F.max('movieCount')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(max(movieCount)=4)],\n",
       " [Row(max(movieCount)=7)],\n",
       " [Row(max(movieCount)=17)],\n",
       " [Row(max(movieCount)=21)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_window = Window().orderBy('movieCount')\n",
    "test_counts_percentile = test_counts.withColumn('fraction', F.percent_rank().over(count_window))\n",
    "\n",
    "test_counts_percentile.filter(F.col('fraction') <= 0.25).select(F.max('movieCount')).collect(), \\\n",
    "test_counts_percentile.filter(F.col('fraction') <= 0.5).select(F.max('movieCount')).collect(), \\\n",
    "test_counts_percentile.filter(F.col('fraction') <= 0.75).select(F.max('movieCount')).collect(), \\\n",
    "test_counts_percentile.filter(F.col('fraction') <= 0.8).select(F.max('movieCount')).collect(), \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 80% пользователей достаточно рекомендовать около 20 фильмов\n",
    "\n",
    "Будем использовать [Precision, NDCG] @ [1, 5, 10, 20] и MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "KS = [1, 5, 10, 20]\n",
    "\n",
    "def get_metrics(predictions_df, gt_df):\n",
    "    labels = gt_df \\\n",
    "        .groupBy('userId') \\\n",
    "        .agg(F.collect_set('movieId').alias('labels'))\n",
    "    \n",
    "    ranking_metrics = RankingMetrics(\n",
    "        labels.join(predictions_df, on='userId') \\\n",
    "        .select('predictions', 'labels') \\\n",
    "        .rdd\n",
    "    )\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    for k in KS:\n",
    "        metrics[f'Precision@{k}'] = ranking_metrics.precisionAt(k)\n",
    "        metrics[f'NDCG@{k}'] = ranking_metrics.ndcgAt(k)\n",
    "    metrics['MAP'] = ranking_metrics.meanAveragePrecision\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Теперь мы можем перейти к формулировке задачи в терминах машинного обучения.\n",
    "\n",
    "Одна из формулировок, к которой мы сведем нашу задачу - **Matrix Completetion**. Данную задачу будем решать с помощью `ALS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ALS](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html#explicit-vs-implicit-feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, rank=100, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите для выбранных вами фильмов топ-20 наиболее похожих фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_vectors = model.itemFactors \\\n",
    "    .select(F.col('id').alias('movieId'), F.col('features').alias('vector'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + MOVIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, movieId: string, title: string, genres: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|              vector|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|    380|    True Lies (1994)|Action|Adventure|...|[0.064716086, -0....|\n",
      "|    231|Dumb & Dumber (Du...|    Adventure|Comedy|[0.040393606, -0....|\n",
      "|    293|Léon: The Profess...|Action|Crime|Dram...|[-0.060172215, -0...|\n",
      "|    344|Ace Ventura: Pet ...|              Comedy|[0.07797792, -0.0...|\n",
      "|    364|Lion King, The (1...|Adventure|Animati...|[0.24805997, -0.0...|\n",
      "|    296| Pulp Fiction (1994)|Comedy|Crime|Dram...|[-0.097767875, -0...|\n",
      "|    356| Forrest Gump (1994)|Comedy|Drama|Roma...|[0.07929398, -0.0...|\n",
      "|    367|    Mask, The (1994)|Action|Comedy|Cri...|[-0.009585199, -0...|\n",
      "|    377|        Speed (1994)|Action|Romance|Th...|[0.03229766, -0.0...|\n",
      "|    318|Shawshank Redempt...|         Crime|Drama|[-0.07772393, -0....|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_ids = [356, 318, 296, 364, 380, 377, 344, 367, 293, 231]\n",
    "\n",
    "picked_movies = movies_df \\\n",
    "    .filter(F.col('movieId').isin(movie_ids)) \\\n",
    "    .join(item_vectors, on='movieId')\n",
    "\n",
    "picked_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(v, u):\n",
    "    return float(cosine(v, u))\n",
    "    \n",
    "cos_udf = F.udf(cos_dist, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не делать вывод слишком большим, посмотрим на 10 (а не 20) близких фильмов для 10 популярных фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies similar to Forrest Gump (1994)\n",
      "0.040 | Green Mile, The (1999)\n",
      "0.050 | Scent of a Woman (1992)\n",
      "0.053 | Rain Man (1988)\n",
      "0.055 | Shawshank Redemption, The (1994)\n",
      "0.056 | Few Good Men, A (1992)\n",
      "0.056 | Cast Away (2000)\n",
      "0.060 | Good Will Hunting (1997)\n",
      "0.063 | As Good as It Gets (1997)\n",
      "0.064 | Dances with Wolves (1990)\n",
      "0.065 | Regarding Henry (1991)\n",
      "\n",
      "Movies similar to Shawshank Redemption, The (1994)\n",
      "0.030 | Schindler's List (1993)\n",
      "0.030 | Rain Man (1988)\n",
      "0.032 | Bronx Tale, A (1993)\n",
      "0.033 | As Good as It Gets (1997)\n",
      "0.035 | Good Will Hunting (1997)\n",
      "0.037 | Glory (1989)\n",
      "0.040 | Philadelphia (1993)\n",
      "0.041 | Good Morning, Vietnam (1987)\n",
      "0.041 | Usual Suspects, The (1995)\n",
      "0.041 | Lean on Me (1989)\n",
      "\n",
      "Movies similar to Pulp Fiction (1994)\n",
      "0.026 | Reservoir Dogs (1992)\n",
      "0.039 | True Romance (1993)\n",
      "0.039 | Trainspotting (1996)\n",
      "0.052 | Shallow Grave (1994)\n",
      "0.057 | High Noon (2000)\n",
      "0.058 | Clerks (1994)\n",
      "0.059 | Macross: Flash Back 2012 (1987)\n",
      "0.059 | Macross Zero (2002)\n",
      "0.059 | Macross Frontier: The False Songstress (2009)\n",
      "0.061 | Full Metal Jacket (1987)\n",
      "\n",
      "Movies similar to Lion King, The (1994)\n",
      "0.017 | Aladdin (1992)\n",
      "0.038 | Tarzan (1999)\n",
      "0.040 | Hercules (1997)\n",
      "0.046 | Beauty and the Beast (1991)\n",
      "0.048 | Mulan (1998)\n",
      "0.050 | American Tail, An (1986)\n",
      "0.052 | Fox and the Hound, The (1981)\n",
      "0.054 | Little Mermaid, The (1989)\n",
      "0.056 | Stagecoach: The Texas Jack Story (2016)\n",
      "0.057 | Prince of Egypt, The (1998)\n",
      "\n",
      "Movies similar to True Lies (1994)\n",
      "0.033 | Lethal Weapon (1987)\n",
      "0.035 | Die Hard 2 (1990)\n",
      "0.036 | Lethal Weapon 2 (1989)\n",
      "0.036 | Under Siege (1992)\n",
      "0.045 | Speed (1994)\n",
      "0.049 | Die Hard: With a Vengeance (1995)\n",
      "0.050 | GoldenEye (1995)\n",
      "0.050 | Expecting Love (Mala wielka milosc) (2008)\n",
      "0.051 | Rock, The (1996)\n",
      "0.053 | Die Hard (1988)\n",
      "\n",
      "Movies similar to Speed (1994)\n",
      "0.043 | Executive Decision (1996)\n",
      "0.045 | True Lies (1994)\n",
      "0.048 | Twister (1996)\n",
      "0.049 | Cliffhanger (1993)\n",
      "0.053 | Under Siege (1992)\n",
      "0.055 | Broken Arrow (1996)\n",
      "0.056 | Air Force One (1997)\n",
      "0.060 | Fugitive, The (1993)\n",
      "0.065 | In the Line of Fire (1993)\n",
      "0.065 | Eraser (1996)\n",
      "\n",
      "Movies similar to Ace Ventura: Pet Detective (1994)\n",
      "0.038 | Ace Ventura: When Nature Calls (1995)\n",
      "0.046 | Dumb & Dumber (Dumb and Dumber) (1994)\n",
      "0.077 | Tommy Boy (1995)\n",
      "0.092 | Happy Gilmore (1996)\n",
      "0.095 | Billy Madison (1995)\n",
      "0.095 | Me, Myself & Irene (2000)\n",
      "0.097 | Austin Powers: The Spy Who Shagged Me (1999)\n",
      "0.105 | Waterboy, The (1998)\n",
      "0.113 | Wayne's World 2 (1993)\n",
      "0.114 | Austin Powers in Goldmember (2002)\n",
      "\n",
      "Movies similar to Mask, The (1994)\n",
      "0.056 | Liar Liar (1997)\n",
      "0.088 | Houseguest (1994)\n",
      "0.095 | Nothing to Lose (1994)\n",
      "0.097 | Twins (1988)\n",
      "0.099 | Jumanji (1995)\n",
      "0.100 | I Am Road Comic (2014)\n",
      "0.100 | Last Action Hero (1993)\n",
      "0.101 | Hot Shots! (1991)\n",
      "0.102 | Piggy (2012)\n",
      "0.102 | Resurrecting the Street Walker (2009)\n",
      "\n",
      "Movies similar to Léon: The Professional (a.k.a. The Professional) (Léon) (1994)\n",
      "0.016 | High Tech Soul: The Creation of Techno Music (2006)\n",
      "0.016 | Pioneers of Electronic Music, Volume 1: Richie Hawtin (2006)\n",
      "0.016 | Little World (2013)\n",
      "0.018 | Siivoton Juttu (1997)\n",
      "0.019 | Name of the Rose, The (Name der Rose, Der) (1986)\n",
      "0.019 | Chiara Ferragni - Unposted (2019)\n",
      "0.019 | The Whispering Shadow (1933)\n",
      "0.019 | Professional, The (Le professionnel) (1981)\n",
      "0.020 | Rageh Inside Iran (2007)\n",
      "0.020 | Eugenio (2018)\n",
      "\n",
      "Movies similar to Dumb & Dumber (Dumb and Dumber) (1994)\n",
      "0.046 | Ace Ventura: Pet Detective (1994)\n",
      "0.075 | Tommy Boy (1995)\n",
      "0.094 | Ace Ventura: When Nature Calls (1995)\n",
      "0.096 | Jackass: The Movie (2002)\n",
      "0.109 | Billy Madison (1995)\n",
      "0.113 | Happy Gilmore (1996)\n",
      "0.116 | Me, Myself & Irene (2000)\n",
      "0.122 | Jackass Number Two (2006)\n",
      "0.122 | Kingpin (1996)\n",
      "0.127 | Dirty Work (1998)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SIMILAR = 10\n",
    "\n",
    "movie_vectors = movies_df \\\n",
    "    .join(item_vectors, on='movieId')\n",
    "\n",
    "for movie_id in movie_ids:\n",
    "    picked_movie = picked_movies \\\n",
    "        .filter(F.col('movieId') == movie_id)\n",
    "    \n",
    "    print(f\"Movies similar to {picked_movie.select('title').collect()[0]['title']}\")\n",
    "    \n",
    "    picked_movie = picked_movie \\\n",
    "        .select(F.col('movieId').alias('pickedId'), F.col('vector').alias('pickedVector'))\n",
    "     \n",
    "    movie_similarity = movie_vectors \\\n",
    "        .join(picked_movie, on=F.col('movieId') != F.col('pickedId')) \\\n",
    "        .withColumn('dist', cos_udf(F.col('vector'), F.col('pickedVector'))) \\\n",
    "        .select('title', 'dist')\n",
    "        \n",
    "    similar_window = Window().orderBy('dist')\n",
    "    \n",
    "    similar_movies = movie_similarity \\\n",
    "        .withColumn('rank', F.rank().over(similar_window)) \\\n",
    "        .filter(F.col('rank') <= SIMILAR) \\\n",
    "        .select('title', 'dist')\n",
    "    \n",
    "    for row in similar_movies.collect():\n",
    "        print(f\"{row['dist']:.3f} | {row['title']}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваша формулировка\n",
    "\n",
    "На лекции было еще несколько ML формулировок задачи рекомендаций. Выберете одну из них и реализуйте метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_window = Window().partitionBy('userId').orderBy('timestamp')\n",
    "\n",
    "ratings_df_ranked = train_df \\\n",
    "    .withColumn('rank', F.rank().over(rank_window)) \\\n",
    "    .select('userId', 'movieId', 'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_pairs = ratings_df_ranked \\\n",
    "    .select(\n",
    "        F.col('userId').alias('userIdInit'), \n",
    "        F.col('movieId').alias('movieIdInit'), \n",
    "        F.col('rank').alias('rankInit')\n",
    "    ) \\\n",
    "    .join(ratings_df_ranked, on=[F.col('userIdInit') == F.col('userId'), F.col('rankInit') < F.col('rank')]) \\\n",
    "    .withColumn('deltaRank', 1 / (F.col('rank') - F.col('rankInit'))) \\\n",
    "    .select(F.col('movieIdInit').alias('movie1'), F.col('movieId').alias('movie2'), 'deltaRank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_weights = movie_pairs \\\n",
    "    .groupBy('movie1', 'movie2') \\\n",
    "    .agg(F.sum('deltaRank').alias('weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|movie1|movie2|            weight|\n",
      "+------+------+------------------+\n",
      "|   260|   912| 466.1012564665204|\n",
      "| 58559| 88810|  53.9652675456479|\n",
      "|  1198|  1136| 1050.577466623817|\n",
      "|  1198| 68954| 633.5856414520265|\n",
      "|  2571| 79132| 3631.446557388445|\n",
      "|  1213|   593|1233.1577751746715|\n",
      "|  1136|  2176|19.164468015686655|\n",
      "|  1089|  1086| 56.99029650317705|\n",
      "|  1217|   912|42.964930673210574|\n",
      "|   608|  3000| 146.1083700873866|\n",
      "|   608|  1250| 186.4604159665512|\n",
      "|  3000| 63082| 57.08382250426676|\n",
      "|116897|    32|10.220482535321636|\n",
      "|  3462|  1199|22.004069106008863|\n",
      "|  1250|  1207| 140.0087397216013|\n",
      "|  1276|  1080| 112.9396771255049|\n",
      "|109487| 44191|290.10415203679185|\n",
      "|  3949|   953| 52.37575286743941|\n",
      "|  1199| 77455|15.089828307796683|\n",
      "| 78499|112556|134.64502293663443|\n",
      "+------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transition_weights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|movieId|               title|            weight|\n",
      "+-------+--------------------+------------------+\n",
      "|   1917|   Armageddon (1998)| 1127.759743527475|\n",
      "|   2710|Blair Witch Proje...|1015.6232804455163|\n",
      "|   4718|American Pie 2 (2...| 992.4110949053967|\n",
      "|   1517|Austin Powers: In...| 821.4880284492772|\n",
      "|   2683|Austin Powers: Th...| 688.9186545850187|\n",
      "|   2355|Bug's Life, A (1998)| 632.7361547343305|\n",
      "|   1208|Apocalypse Now (1...| 587.9580275797273|\n",
      "|   3114|  Toy Story 2 (1999)| 533.0524642263117|\n",
      "|      2|      Jumanji (1995)| 516.5229523065678|\n",
      "|   1682|Truman Show, The ...| 499.3032730629892|\n",
      "|   1206|Clockwork Orange,...|481.63108806513225|\n",
      "|   2716|Ghostbusters (a.k...|462.56122445125084|\n",
      "|   2997|Being John Malkov...| 445.2078232850714|\n",
      "|   1101|      Top Gun (1986)|443.51080322393676|\n",
      "|   2959|   Fight Club (1999)|432.34083465151133|\n",
      "|    223|       Clerks (1994)|424.17002996188955|\n",
      "|   2918|Ferris Bueller's ...| 383.2630078221996|\n",
      "|   4022|    Cast Away (2000)|376.77322573650554|\n",
      "|    104|Happy Gilmore (1996)| 375.9474184554659|\n",
      "|   2694|    Big Daddy (1999)|375.65042400612424|\n",
      "+-------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Where does American Pie lead us?\n",
    "\n",
    "transition_weights \\\n",
    "    .filter(F.col('movie1') == 2706) \\\n",
    "    .join(movies_df, on=F.col('movie2') == F.col('movieId')) \\\n",
    "    .select('movieId', 'title', 'weight') \\\n",
    "    .sort(F.col('weight').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialRulesModel:\n",
    "    \n",
    "    def __init__(self, ratings):\n",
    "        last_rating_window = Window().partitionBy('userId').orderBy(F.col('timestamp').desc())\n",
    "\n",
    "        self.last_user_ratings = ratings \\\n",
    "            .withColumn('timestampOrder', F.row_number().over(last_rating_window)) \\\n",
    "            .filter(F.col('timestampOrder') == 1) \\\n",
    "            .select('userId', 'movieId') \\\n",
    "            .cache()\n",
    "        \n",
    "        transition_weights = self.compute_transition_weights(ratings)\n",
    "        \n",
    "        top_transition_window = Window().partitionBy('movie1').orderBy(F.col('weight').desc())\n",
    "        \n",
    "        self.ranked_transition_weights = transition_weights \\\n",
    "            .withColumn('rank', F.row_number().over(top_transition_window)) \\\n",
    "            .drop('weight') \\\n",
    "            .cache()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_transition_weights(ratings):\n",
    "        rank_window = Window().partitionBy('userId').orderBy('timestamp')\n",
    "\n",
    "        ratings_ranked = ratings \\\n",
    "            .withColumn('rank', F.rank().over(rank_window)) \\\n",
    "            .select('userId', 'movieId', 'rank')\n",
    "        \n",
    "        movie_pairs = ratings_df_ranked \\\n",
    "            .select(\n",
    "                F.col('userId').alias('userIdInit'), \n",
    "                F.col('movieId').alias('movieIdInit'), \n",
    "                F.col('rank').alias('rankInit')\n",
    "            ) \\\n",
    "            .join(ratings_df_ranked, on=[F.col('userIdInit') == F.col('userId'), F.col('rankInit') < F.col('rank')]) \\\n",
    "            .withColumn('deltaRank', 1 / (F.col('rank') - F.col('rankInit'))) \\\n",
    "            .select(F.col('movieIdInit').alias('movie1'), F.col('movieId').alias('movie2'), 'deltaRank')\n",
    "        \n",
    "        return movie_pairs \\\n",
    "            .groupBy('movie1', 'movie2') \\\n",
    "            .agg(F.sum('deltaRank').alias('weight'))\n",
    "        \n",
    "    def recommend(self, k, users):\n",
    "        top_k_transitions = self.ranked_transition_weights \\\n",
    "            .filter(F.col('rank') <= k) \\\n",
    "            .drop('rank')\n",
    "        \n",
    "        return self.last_user_ratings \\\n",
    "            .join(users, on='userId') \\\n",
    "            .join(top_k_transitions, on=F.col('movieId') == F.col('movie1')) \\\n",
    "            .select('userId', F.col('movie2').alias('recommendedMovieid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = SequentialRulesModel(train_df.sample(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "Сравните реализованные методы с помощью выбранных метрик. Не забывайте про оптимизацию гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = valid_df.select('userId').distinct()\n",
    "test_users = test_df.select('userId').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_als_predictions(model, user_set):\n",
    "    als_predictions = model.recommendForUserSubset(user_set, max(KS)).cache()    \n",
    "    def extract_predictions(recommendations):\n",
    "        return [rec.movieId for rec in recommendations]\n",
    "\n",
    "    extract_predictions_udf = F.udf(extract_predictions, ArrayType(IntegerType()))\n",
    "\n",
    "    als_predictions = als_predictions \\\n",
    "        .withColumn('predictions', extract_predictions_udf(F.col('recommendations'))) \\\n",
    "        .drop('recommendations')\n",
    "    \n",
    "    return als_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_srm_predictions(model, user_set):\n",
    "    srm_predictions = model.recommend(max(KS), user_set).cache()\n",
    "\n",
    "    srm_predictions = srm_predictions \\\n",
    "        .groupBy('userId') \\\n",
    "        .agg(F.collect_set('recommendedMovieId').alias('predictions'))\n",
    "    \n",
    "    return srm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': 0.02582970415978375,\n",
       " 'NDCG@1': 0.02582970415978375,\n",
       " 'Precision@5': 0.026330279821795054,\n",
       " 'NDCG@5': 0.027141763746588003,\n",
       " 'Precision@10': 0.025913550583170644,\n",
       " 'NDCG@10': 0.029724978636808033,\n",
       " 'Precision@20': 0.025832207038093805,\n",
       " 'NDCG@20': 0.037010902602625434,\n",
       " 'MAP': 0.009752179444347512}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srm_predictions = get_srm_predictions(sequential_model, valid_users)\n",
    "get_metrics(srm_predictions, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': 2.523611540475576e-05,\n",
       " 'NDCG@1': 2.5236115404755766e-05,\n",
       " 'Precision@5': 1.7665280783329033e-05,\n",
       " 'NDCG@5': 2.097586715394493e-05,\n",
       " 'Precision@10': 1.0094446161902298e-05,\n",
       " 'NDCG@10': 1.5447205721427435e-05,\n",
       " 'Precision@20': 1.2618057702377883e-05,\n",
       " 'NDCG@20': 2.4613230085202504e-05,\n",
       " 'MAP': 4.766596847810347e-06}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_predictions = get_als_predictions(model, valid_users)\n",
    "get_metrics(als_predictions, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': 0.01951578868508783,\n",
       " 'NDCG@1': 0.01951578868508783,\n",
       " 'Precision@5': 0.019390927593052724,\n",
       " 'NDCG@5': 0.020010767693939967,\n",
       " 'Precision@10': 0.019399667869495176,\n",
       " 'NDCG@10': 0.02208205044223997,\n",
       " 'Precision@20': 0.019334115796176762,\n",
       " 'NDCG@20': 0.02735393132782173,\n",
       " 'MAP': 0.006738204974498579}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srm_predictions = get_srm_predictions(sequential_model, test_users)\n",
    "get_metrics(srm_predictions, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': 0.0,\n",
       " 'NDCG@1': 0.0,\n",
       " 'Precision@5': 6.295524511624691e-06,\n",
       " 'NDCG@5': 6.214627880677473e-06,\n",
       " 'Precision@10': 6.295524511624688e-06,\n",
       " 'NDCG@10': 6.697675422241314e-06,\n",
       " 'Precision@20': 8.184181865112098e-06,\n",
       " 'NDCG@20': 1.2323547312328835e-05,\n",
       " 'MAP': 1.692921712081672e-06}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_predictions = get_als_predictions(model, test_users)\n",
    "get_metrics(als_predictions, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
