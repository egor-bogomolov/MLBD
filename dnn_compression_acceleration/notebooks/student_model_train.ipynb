{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Student-Model\" data-toc-modified-id=\"Student-Model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Student Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-processing\" data-toc-modified-id=\"Data-processing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data processing</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Model\" data-toc-modified-id=\"Define-Model-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Define Model</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Train</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#ROC-AUC\" data-toc-modified-id=\"ROC-AUC-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>ROC AUC</a></span></li><li><span><a href=\"#Compression-rate\" data-toc-modified-id=\"Compression-rate-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Compression rate</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Model\n",
    "\n",
    "\n",
    "Нужно обучть небольшую модель на [soft таргетах](https://drive.google.com/file/d/1tBbPOUT-Ow9f3zTDApykGXYwt-KslYle/view?usp=sharing)  модели учителя, которая не сильно уступала бы в качестве учителю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from category_encoders import CatBoostEncoder, TargetEncoder\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names, build_input_features\n",
    "from deepctr_torch.models.dcn import DCN\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/criteo'\n",
    "\n",
    "TRAIN_DATA = os.path.join(DATA_PATH, 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Данные на Train/Validation/Test нужно разбить как 80/10/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loading part was copied from the teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 26)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_features_indices = [i for i in range(1, 14)]\n",
    "sparse_features_indices = [i for i in range(14, 40)]\n",
    "\n",
    "dense_features = ['c{}'.format(i) for i in dense_features_indices]\n",
    "sparse_features = ['c{}'.format(i) for i in sparse_features_indices]\n",
    "\n",
    "len(dense_features_indices), len(sparse_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egor/.local/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(TRAIN_DATA, index_col='id')\n",
    "data.rename(columns=dict([(col, col[1:] if col[0] == '_' else col) for col in data.columns]), inplace=True)\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )\n",
    "\n",
    "hard_target = ['c0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_targets = pd.read_csv('soft_targets_full.csv', index_col='id', squeeze=True)\n",
    "\n",
    "data['c0_soft'] = soft_targets\n",
    "\n",
    "soft_target = ['c0_soft']\n",
    "targets = hard_target + soft_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>...</th>\n",
       "      <th>c31</th>\n",
       "      <th>c32</th>\n",
       "      <th>c33</th>\n",
       "      <th>c34</th>\n",
       "      <th>c35</th>\n",
       "      <th>c36</th>\n",
       "      <th>c37</th>\n",
       "      <th>c38</th>\n",
       "      <th>c39</th>\n",
       "      <th>c0_soft</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5f8f18f</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>f3ddd519</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>b34f3128</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.532062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1548.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1f868fdd</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>a458ea53</td>\n",
       "      <td>7eee76d1</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>9af06ad9</td>\n",
       "      <td>9d93af03</td>\n",
       "      <td>cdfe5ab7</td>\n",
       "      <td>0.483268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1304f63b</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>07b2853e</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>94bde4f2</td>\n",
       "      <td>010f6491</td>\n",
       "      <td>09b76f8d</td>\n",
       "      <td>0.126496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>88.0</td>\n",
       "      <td>319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>bbf70d82</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>16e2e3b3</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>d859b4dd</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.750299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6550.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>...</td>\n",
       "      <td>fa0643ee</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>0094bc78</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>29ece3ed</td>\n",
       "      <td>001f3601</td>\n",
       "      <td>402185f3</td>\n",
       "      <td>0.784883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    c0    c1   c2    c3    c4      c5    c6    c7    c8     c9  ...       c31  \\\n",
       "id                                                              ...             \n",
       "12   1   0.0   -1   0.0   0.0  1465.0   0.0  17.0   0.0    4.0  ...  e5f8f18f   \n",
       "26   1   0.0    1  20.0  16.0  1548.0  93.0  42.0  32.0  912.0  ...  1f868fdd   \n",
       "39   0   8.0    0  15.0  20.0   115.0  24.0   8.0  23.0   24.0  ...  1304f63b   \n",
       "41   1  88.0  319   0.0   4.0     5.0   4.0  89.0  40.0   88.0  ...  bbf70d82   \n",
       "85   0   0.0   53   0.0  10.0  6550.0  98.0  34.0  11.0  349.0  ...  fa0643ee   \n",
       "\n",
       "         c32       c33       c34 c35       c36       c37       c38       c39  \\\n",
       "id                                                                             \n",
       "12        -1        -1  f3ddd519  -1  32c7478e  b34f3128        -1        -1   \n",
       "26  21ddcdc9  a458ea53  7eee76d1  -1  32c7478e  9af06ad9  9d93af03  cdfe5ab7   \n",
       "39  21ddcdc9  b1252a9d  07b2853e  -1  32c7478e  94bde4f2  010f6491  09b76f8d   \n",
       "41        -1        -1  16e2e3b3  -1  32c7478e  d859b4dd        -1        -1   \n",
       "85  21ddcdc9  b1252a9d  0094bc78  -1  32c7478e  29ece3ed  001f3601  402185f3   \n",
       "\n",
       "     c0_soft  \n",
       "id            \n",
       "12  0.532062  \n",
       "26  0.483268  \n",
       "39  0.126496  \n",
       "41  0.750299  \n",
       "85  0.784883  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3664931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing the categorial features, let's split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2931944 366493 366494\n"
     ]
    }
   ],
   "source": [
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "train, test = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "validation, test = train_test_split(test, test_size=0.5, shuffle=False)\n",
    "\n",
    "print(len(train), len(validation), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features_dims = {feat: len(data[feat].unique()) for feat in sparse_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c14': 1445,\n",
       " 'c15': 556,\n",
       " 'c16': 1130758,\n",
       " 'c17': 360209,\n",
       " 'c18': 304,\n",
       " 'c19': 21,\n",
       " 'c20': 11845,\n",
       " 'c21': 631,\n",
       " 'c22': 3,\n",
       " 'c23': 49223,\n",
       " 'c24': 5194,\n",
       " 'c25': 985420,\n",
       " 'c26': 3157,\n",
       " 'c27': 26,\n",
       " 'c28': 11588,\n",
       " 'c29': 715441,\n",
       " 'c30': 10,\n",
       " 'c31': 4681,\n",
       " 'c32': 2029,\n",
       " 'c33': 4,\n",
       " 'c34': 870796,\n",
       " 'c35': 17,\n",
       " 'c36': 15,\n",
       " 'c37': 87605,\n",
       " 'c38': 84,\n",
       " 'c39': 58187}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_features_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High cardinality features\n",
    "\n",
    "We have about 3.6M samples in the dataset. For categorial features with high cardinality there are only a few training examples per each category. It is unlikely that we can learn meaningful high-dimensional embeddings for such categories. Also, such embeddings will consume a lot of memory, even if we apply hashing trick and reduce the number of distinct embedding vectors to 50,000 as it was done for the teacher model. Let's take another approach and encode them with some cool supervised techniques (that's why we split the data beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_features = [feat for feat, cnt in sparse_features_dims.items() if cnt >= 40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c16', 'c17', 'c23', 'c25', 'c29', 'c34', 'c37', 'c39']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_cardinality_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egor/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/egor/.local/lib/python3.6/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "enc = CatBoostEncoder(cols=high_cardinality_features, verbose=1)\n",
    "enc.fit(train[high_cardinality_features], train[hard_target])\n",
    "\n",
    "catboost_features = [f'{feat}_catboost' for feat in high_cardinality_features]\n",
    "\n",
    "for df in [train, validation, test]:\n",
    "    for feat in catboost_features:\n",
    "        df[feat] = 0\n",
    "    df.loc[:, catboost_features] = enc.transform(df.loc[:, high_cardinality_features]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egor/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "enc = TargetEncoder(cols=high_cardinality_features, verbose=1)\n",
    "enc.fit(train[high_cardinality_features], train[hard_target])\n",
    "\n",
    "target_features = [f'{feat}_target' for feat in high_cardinality_features]\n",
    "\n",
    "for df in [train, validation, test]:\n",
    "    for feat in target_features:\n",
    "        df[feat] = 0\n",
    "    df.loc[:, target_features] = enc.transform(df.loc[:, high_cardinality_features]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, validation, test]:\n",
    "    df = df.drop(high_cardinality_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = dense_features + catboost_features + target_features\n",
    "sparse_features = [feat for feat in sparse_features if feat not in high_cardinality_features]\n",
    "sparse_features_dims = {feat: dim for feat, dim in sparse_features_dims.items() if feat not in high_cardinality_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low cardinality features\n",
    "\n",
    "Now let's deal with low (kind of) cardinality features. For them we will use label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in sparse_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data.loc[:, feat])\n",
    "\n",
    "    for df in [train, validation, test]:\n",
    "        df.loc[:, feat] = le.transform(df.loc[:, feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>...</th>\n",
       "      <th>c37_catboost</th>\n",
       "      <th>c39_catboost</th>\n",
       "      <th>c16_target</th>\n",
       "      <th>c17_target</th>\n",
       "      <th>c23_target</th>\n",
       "      <th>c25_target</th>\n",
       "      <th>c29_target</th>\n",
       "      <th>c34_target</th>\n",
       "      <th>c37_target</th>\n",
       "      <th>c39_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260759</td>\n",
       "      <td>0.267092</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.360034</td>\n",
       "      <td>0.370765</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.365178</td>\n",
       "      <td>0.365178</td>\n",
       "      <td>0.260760</td>\n",
       "      <td>0.267092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>0.047188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496607</td>\n",
       "      <td>0.506555</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.555449</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.497268</td>\n",
       "      <td>0.507289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156868</td>\n",
       "      <td>0.313736</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.030390</td>\n",
       "      <td>0.211873</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.143134</td>\n",
       "      <td>0.323989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0.015238</td>\n",
       "      <td>0.014591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230221</td>\n",
       "      <td>0.267092</td>\n",
       "      <td>0.208054</td>\n",
       "      <td>0.230209</td>\n",
       "      <td>0.352697</td>\n",
       "      <td>0.211798</td>\n",
       "      <td>0.230209</td>\n",
       "      <td>0.211798</td>\n",
       "      <td>0.230209</td>\n",
       "      <td>0.267092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011403</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.018058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383206</td>\n",
       "      <td>0.311325</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.217176</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    c0        c1        c2        c3        c4        c5        c6        c7  \\\n",
       "id                                                                             \n",
       "12   1  0.000000  0.000091  0.000000  0.000000  0.000558  0.000000  0.001717   \n",
       "26   1  0.000000  0.000181  0.000305  0.018244  0.000590  0.000798  0.004242   \n",
       "39   0  0.001385  0.000136  0.000229  0.022805  0.000044  0.000206  0.000808   \n",
       "41   1  0.015238  0.014591  0.000000  0.004561  0.000002  0.000034  0.008989   \n",
       "85   0  0.000000  0.002537  0.000000  0.011403  0.002497  0.000841  0.003434   \n",
       "\n",
       "          c8        c9  ...  c37_catboost  c39_catboost  c16_target  \\\n",
       "id                      ...                                           \n",
       "12  0.000000  0.000207  ...      0.260759      0.267092    0.254942   \n",
       "26  0.006335  0.047188  ...      0.496607      0.506555    0.254942   \n",
       "39  0.004554  0.001242  ...      0.156868      0.313736    0.254942   \n",
       "41  0.007919  0.004553  ...      0.230221      0.267092    0.208054   \n",
       "85  0.002178  0.018058  ...      0.383206      0.311325    0.254942   \n",
       "\n",
       "    c17_target  c23_target  c25_target c29_target c34_target  c37_target  \\\n",
       "id                                                                         \n",
       "12    0.360034    0.370765    0.254942   0.365178   0.365178    0.260760   \n",
       "26    0.660000    0.555449    0.254942   0.254942   0.254942    0.497268   \n",
       "39    0.030390    0.211873    0.254942   0.254942   0.254942    0.143134   \n",
       "41    0.230209    0.352697    0.211798   0.230209   0.211798    0.230209   \n",
       "85    0.312500    0.217176    0.254942   0.254942   0.254942    0.384615   \n",
       "\n",
       "    c39_target  \n",
       "id              \n",
       "12    0.267092  \n",
       "26    0.507289  \n",
       "39    0.323989  \n",
       "41    0.267092  \n",
       "85    0.312500  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, prepare input for the model (mostly copied from the teacher model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, \n",
    "                                     vocabulary_size=vocab_size, \n",
    "                                     embedding_dim=min(int(6 * (vocab_size) ** (0.25)), 100), \n",
    "                                     use_hash=False, dtype='string') \n",
    "                          for feat, vocab_size in sparse_features_dims.items()] + \\\n",
    "                        [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_input(df):\n",
    "    return {name: (pd.core.series.Series(df[name]) if name in sparse_features else np.array(df[name])) for name in feature_names}\n",
    "\n",
    "\n",
    "train_model_input = gen_model_input(train)\n",
    "validation_model_input = gen_model_input(validation)\n",
    "test_model_input = gen_model_input(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_targets = train[targets]\n",
    "validation_model_targets = validation[targets]\n",
    "test_model_targets = test[targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Можно также использовать Pruning и/или Quantinization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same model as the teacher (DCN) but with smaller number of parameters.\n",
    "\n",
    "To do so, we should redefine the loss function to use both soft and hard targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor([0.1, 0.9])\n",
    "\n",
    "def distillation_loss(y_pred, y_true, reduction='sum'):\n",
    "    y_pred = y_pred.unsqueeze(-1).expand(-1, 2)\n",
    "    loss = F.binary_cross_entropy(y_pred, y_true, reduction='none').sum(axis=0)\n",
    "    weighted_loss = torch.mul(loss, weight).sum()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true[:, 0], y_pred)\n",
    "\n",
    "def metric_distillation_loss(y_true, y_pred):\n",
    "    return distillation_loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN(linear_feature_columns, dnn_feature_columns, cross_num=2,\n",
    "            dnn_hidden_units=(128, 64), l2_reg_linear=0, l2_reg_embedding=0,\n",
    "            l2_reg_cross=0, l2_reg_dnn=0, init_std=0.0001, seed=1024, \n",
    "            dnn_use_bn=True, dnn_activation='relu', task='binary')\n",
    "\n",
    "model.compile(\"adam\", distillation_loss, metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.metrics['auc'] = metric_auc\n",
    "# model.metrics['distillation_loss'] = metric_distillation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The implementation in the library works incorrectly due to issues with reg loss. \n",
    "# I copied the training part from the lib and fixed it a little bit\n",
    "\n",
    "# model.fit(train_model_input, train_model_targets.values[:, 0], \n",
    "#           epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 1\n",
    "EVAL_BATCHES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(x, y, batch_size=BATCH_SIZE):\n",
    "    if isinstance(x, dict):\n",
    "        x = [x[feature] for feature in build_input_features(linear_feature_columns + dnn_feature_columns)]\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if len(x[i].shape) == 1:\n",
    "            x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "    \n",
    "    tensor_data = Data.TensorDataset(torch.from_numpy(np.concatenate(x, axis=-1)), torch.from_numpy(y))\n",
    "        \n",
    "    loader = DataLoader(dataset=tensor_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = prepare_data_loader(train_model_input, train_model_targets.values, BATCH_SIZE)\n",
    "val_loader = prepare_data_loader(validation_model_input, validation_model_targets.values, BATCH_SIZE)\n",
    "test_loader = prepare_data_loader(test_model_input, test_model_targets.values, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, loader, n_samples, batch_size=BATCH_SIZE):\n",
    "    loss_cur = 0\n",
    "    cnt = 0\n",
    "\n",
    "    val_pred = np.empty(n_samples)\n",
    "    val_true = np.empty(n_samples)\n",
    "\n",
    "    model.eval()\n",
    "    for index, (x_val, y_val) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            x = x_val.float()\n",
    "            y = y_val.float()\n",
    "\n",
    "            y_pred = model(x).squeeze()\n",
    "\n",
    "            val_pred[index * batch_size:(index + 1) * batch_size] = y_pred[:]\n",
    "            val_true[index * batch_size:(index + 1) * batch_size] = y[:, 0]\n",
    "\n",
    "            loss_cur += loss_func(y_pred, y.squeeze(), reduction='sum')\n",
    "            cnt += 1\n",
    "\n",
    "    average_loss = loss_cur / cnt\n",
    "    auc = roc_auc_score(val_true, val_pred)\n",
    "    \n",
    "    return average_loss, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, loss_func, epochs, eval_batches=1000, batch_size=BATCH_SIZE):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Start epoch {epoch + 1}')\n",
    "\n",
    "        loss_cur = 0\n",
    "        model.train()\n",
    "        for index, (x_train, y_train) in enumerate(train_loader):\n",
    "            x = x_train.float()\n",
    "            y = y_train.float()\n",
    "\n",
    "            y_pred = model(x).squeeze()\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss = loss_func(y_pred, y.squeeze(), reduction='sum')\n",
    "\n",
    "            loss_cur += loss.item()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optim.step()\n",
    "\n",
    "            if (index + 1) % eval_batches == 0 or index + 1 == steps_per_epoch:\n",
    "                print(f'Step {index + 1} of {steps_per_epoch}')\n",
    "                print(f'Average train loss: {loss_cur / eval_batches}')\n",
    "                loss_cur = 0\n",
    "                \n",
    "                average_val_loss, val_auc = eval(model, val_loader, len(validation_model_targets))\n",
    "\n",
    "                print(f'Average validation loss: {average_val_loss}')\n",
    "                print(f'Validation ROC AUC: {val_auc}')\n",
    "\n",
    "        print(f'Finished epoch')\n",
    "    \n",
    "    average_test_loss, test_auc = eval(model, test_loader, len(test_model_targets))\n",
    "\n",
    "    print(f'Average test loss: {average_test_loss}')\n",
    "    print(f'Test ROC AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2931944 samples, validate on 366493 samples, 5727 steps per epoch\n"
     ]
    }
   ],
   "source": [
    "loss_func = distillation_loss\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "sample_num = len(train_model_targets)\n",
    "steps_per_epoch = (sample_num - 1) // BATCH_SIZE + 1\n",
    "\n",
    "print(f\"Train on {sample_num} samples, validate on {len(validation_model_targets)} samples, {steps_per_epoch} steps per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1\n",
      "Step 1000 of 5727\n",
      "Average train loss: 225.89276454162598\n",
      "Average validation loss: 228.2914581298828\n",
      "Validation ROC AUC: 0.7869350674827783\n",
      "Step 2000 of 5727\n",
      "Average train loss: 225.7315050201416\n",
      "Average validation loss: 228.20632934570312\n",
      "Validation ROC AUC: 0.787350988959743\n",
      "Step 3000 of 5727\n",
      "Average train loss: 225.21759950256347\n",
      "Average validation loss: 228.2144775390625\n",
      "Validation ROC AUC: 0.7873141901963668\n",
      "Step 4000 of 5727\n",
      "Average train loss: 225.773056640625\n",
      "Average validation loss: 228.10110473632812\n",
      "Validation ROC AUC: 0.7875991977008148\n",
      "Step 5000 of 5727\n",
      "Average train loss: 225.6276220550537\n",
      "Average validation loss: 228.0832977294922\n",
      "Validation ROC AUC: 0.7878775984946009\n",
      "Step 5727 of 5727\n",
      "Average train loss: 163.8836019821167\n",
      "Average validation loss: 228.08460998535156\n",
      "Validation ROC AUC: 0.7876097888764665\n",
      "Finished epoch\n",
      "Start epoch 2\n",
      "Step 1000 of 5727\n",
      "Average train loss: 225.30787113952636\n",
      "Average validation loss: 228.1519775390625\n",
      "Validation ROC AUC: 0.7877692915771772\n",
      "Step 2000 of 5727\n",
      "Average train loss: 225.34938345336914\n",
      "Average validation loss: 228.0811309814453\n",
      "Validation ROC AUC: 0.7877635747880909\n",
      "Step 3000 of 5727\n",
      "Average train loss: 225.28002909851074\n",
      "Average validation loss: 228.13906860351562\n",
      "Validation ROC AUC: 0.7879223328821918\n",
      "Step 4000 of 5727\n",
      "Average train loss: 225.15320442199706\n",
      "Average validation loss: 228.06971740722656\n",
      "Validation ROC AUC: 0.7880743532025247\n",
      "Step 5000 of 5727\n",
      "Average train loss: 225.45968148803712\n",
      "Average validation loss: 228.07595825195312\n",
      "Validation ROC AUC: 0.7876933992310946\n",
      "Step 5727 of 5727\n",
      "Average train loss: 163.56472799682618\n",
      "Average validation loss: 228.0133819580078\n",
      "Validation ROC AUC: 0.7881781869068276\n",
      "Finished epoch\n",
      "Start epoch 3\n",
      "Step 1000 of 5727\n",
      "Average train loss: 225.11089833068849\n",
      "Average validation loss: 228.17471313476562\n",
      "Validation ROC AUC: 0.7880080466685305\n",
      "Step 2000 of 5727\n",
      "Average train loss: 224.90391270446779\n",
      "Average validation loss: 228.1007843017578\n",
      "Validation ROC AUC: 0.7880557526555544\n",
      "Step 3000 of 5727\n",
      "Average train loss: 224.88008740234375\n",
      "Average validation loss: 228.13853454589844\n",
      "Validation ROC AUC: 0.7881015089535224\n",
      "Step 4000 of 5727\n",
      "Average train loss: 225.1214748840332\n",
      "Average validation loss: 228.08993530273438\n",
      "Validation ROC AUC: 0.7878731451646559\n",
      "Step 5000 of 5727\n",
      "Average train loss: 225.18137493896484\n",
      "Average validation loss: 228.04562377929688\n",
      "Validation ROC AUC: 0.7882164613258364\n",
      "Step 5727 of 5727\n",
      "Average train loss: 163.6389923553467\n",
      "Average validation loss: 228.0631561279297\n",
      "Validation ROC AUC: 0.7879004728829453\n",
      "Finished epoch\n",
      "Average test loss: 231.38064575195312\n",
      "Test ROC AUC: 0.7858410887588734\n"
     ]
    }
   ],
   "source": [
    "train(model, optim, loss_func, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1\n",
      "Step 1000 of 5727\n",
      "Average train loss: 224.91948080444337\n",
      "Average validation loss: 229.39315795898438\n",
      "Validation ROC AUC: 0.7891213346828043\n",
      "Step 2000 of 5727\n",
      "Average train loss: 224.81551387023927\n",
      "Average validation loss: 229.46002197265625\n",
      "Validation ROC AUC: 0.788949591428552\n",
      "Step 3000 of 5727\n",
      "Average train loss: 224.64388711547852\n",
      "Average validation loss: 229.48814392089844\n",
      "Validation ROC AUC: 0.7891638395126233\n",
      "Step 4000 of 5727\n",
      "Average train loss: 224.5111068572998\n",
      "Average validation loss: 229.56686401367188\n",
      "Validation ROC AUC: 0.7893397359785075\n",
      "Step 5000 of 5727\n",
      "Average train loss: 224.6653574066162\n",
      "Average validation loss: 229.5581817626953\n",
      "Validation ROC AUC: 0.7889826557033917\n",
      "Step 5727 of 5727\n",
      "Average train loss: 163.3666459197998\n",
      "Average validation loss: 229.51560974121094\n",
      "Validation ROC AUC: 0.7892915225558679\n",
      "Finished epoch\n",
      "Average test loss: 232.68360900878906\n",
      "Test ROC AUC: 0.7872441827415813\n"
     ]
    }
   ],
   "source": [
    "train(model, optim, loss_func, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Наша основная задача получить модель, которая \n",
    "* в терминах ROC AUC не намного хуже модели учителя, и в то же время \n",
    "* сильно меньше по размеру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "Сравним ROC AUC модели ученика с показателем для учителя.\n",
    "\n",
    "ROC AUC учителя: 0.802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'98.155%'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{(0.7872 / 0.802 * 100):.3f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression rate\n",
    "\n",
    "Пусть \n",
    "* $a$ - \\# of the parameters in the original model $M$\n",
    "* $a^{*}$ - \\# of the parameters in compressed model $M^{*}$\n",
    "\n",
    "тогда compression rate is $$\\alpha(M,M^{*}) = \\frac{a}{a^{*}}$$\n",
    "\n",
    "Можно также посчитать comression rate просто как отношение фактических размеров моделей.\n",
    "\n",
    "Размер модели учителя - 168MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_compressed.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.260869565217394"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "168 / 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
